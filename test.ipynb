{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import create_loss as crloss\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "# In[1]:\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os\n",
    "import time\n",
    "from data.my_dataset import MyDataset\n",
    "\n",
    "import fire\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evl_epoch(model,loader):\n",
    "    '''\n",
    "    Args:\n",
    "        model:\n",
    "        loader:\n",
    "        \n",
    "    '''\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # initialize the label and featrs\n",
    "    cles = torch.FloatTensor()\n",
    "    cles = cles.to(device)\n",
    "\n",
    "    featrs = torch.FloatTensor()\n",
    "    featrs = featrs.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx (inputs,labels) in enumerate(loader):\n",
    "        \n",
    "        \n",
    "        # Create vaiables\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        cles = torch.cat((cles, labels), 0)\n",
    "        \n",
    "        # feed the network\n",
    "        ouputs = model(inputs)\n",
    "        featrs = torch.cat((featrs, outputs.data), 0)\n",
    "        \n",
    "        #AUROCs = compute_AUCs(gt, pred)\n",
    "        \n",
    "    return featrs,cles\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def evl(model,train_set,  # model and data\n",
    "          batch_size ,n_epochs,  # others\n",
    "          embed_size,loss,metric,sampler,num_cls,lambda_,   # train\n",
    "          save,result_f,model_f,\n",
    "          wd=0,momentum=0.9,lr=0.001):      # optim\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "        model:\n",
    "        result_f:\n",
    "        data_sets:\n",
    "        \n",
    "        opti_para:\n",
    "        scheduler_para:\n",
    "            \n",
    "    ''' \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Wrap model for multi-GPUs, if necessary\n",
    "    model_wrapper = model\n",
    " \n",
    "    # Start log\n",
    "    with open(os.path.join(save, result_f), 'w') as f:\n",
    "        f.write('epoch,grad,\\n')\n",
    "  \n",
    "    \n",
    "    q_featrs,q_labels = evl_epoch(\n",
    "            model=model_wrapper,\n",
    "            loader=q_set)\n",
    "    \n",
    "    featrs,labels = evl_epoch(\n",
    "        model = model_wrapper,\n",
    "        loader = c_set)\n",
    "    \n",
    "    # find out the nearest :\n",
    "    for i in q_featrs:\n",
    "        #compute the dist between i and featrs\n",
    "        dis_mat = \n",
    "        #  idx of the nearest \n",
    "        idx =\n",
    "        # labels of the k nearest sample\n",
    "        labels\n",
    "        # precision and so no\n",
    "      \n",
    "\n",
    "\n",
    "def test(data,save,\n",
    "         metric,loss,sampler='rand',embed_size=16,num_cls = 196,  lambda_=0, lr=0.001,\n",
    "         model_saved=None,result='r_.csv',\n",
    "         list_file_1 = './cars_test_q.txt',list_file_2 = './cars_test_c.txt',\n",
    "         batch_size=500 ):\n",
    "    '''\n",
    "    Arg:\n",
    "        # data\n",
    "        data: where the data are stored\n",
    "        dataset: the name of the dataset: CIFAR196,etc.\n",
    "        train_size:\n",
    "        test_size:\n",
    "        \n",
    "        # log\n",
    "        save: tr   where the logs are stored\n",
    "        model_saved: trained_model\n",
    "        result:  loss\n",
    "        model_new : \n",
    "        \n",
    "        # trainning para\n",
    "        embed_size: m : 16,32,64\n",
    "        \n",
    "        metric:  'E', 'rE','maha','snr','rM'\n",
    "        \n",
    "        sampler:'dist','npair'\n",
    "        \n",
    "        loss; 'tripl','contras','npair','lifted',\n",
    "        \n",
    "        #margin\n",
    "            \n",
    "        # others\n",
    "        n_epochs :10 tr\n",
    "        batch_size   tr\n",
    "        seed :1\n",
    "        \n",
    "    '''\n",
    "\n",
    "    # dataLoader\n",
    "    q_set = MyDataset(dataroot=data,phase='test',image_list_file =list_file_1 )\n",
    "    c_set = MyDataset(dataroot = data,phase='test',image_list_file=list_file_2)\n",
    "    \n",
    "    #test_loader = DataLoader(dataset=test_set, batch_size=batch_size,shuffle=False, num_workers=4,pin_memory=torch.cuda.is_available())\n",
    "    q_loader = DataLoader(dataset=q_set, batch_size=batch_size,shuffle=False, num_workers=4,pin_memory=torch.cuda.is_available())\n",
    "    c_loader = DataLoader(dataset=c_set, batch_size=batch_size,shuffle=False, num_workers=4,pin_memory=torch.cuda.is_available())\n",
    "    # model_setup\n",
    "    if metric=='maha' or metric == 'rM':\n",
    "        model = models.alexnet(pretrained=False)\n",
    "        inp_fts =  model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(inp_fts, embed_size)\n",
    "        model.load_state_dict(torch.load(os.path.join(save, model_saved),map_location = 'cpu'))\n",
    "        \n",
    "    else:\n",
    "        model = models.alexnet(pretrained=False)\n",
    "        inp_fts =  model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(inp_fts, embed_size)\n",
    "        model.load_state_dict(torch.load(os.path.join(save, model_saved),map_location = 'cpu'))\n",
    "        \n",
    "        \n",
    "    print(model)\n",
    "    # create folder for logging file\n",
    "    # Make save directory\n",
    "    if not os.path.exists(save):\n",
    "        os.makedirs(save)\n",
    "    if not os.path.isdir(save):\n",
    "        raise Exception('%s is not a dir' % save)\n",
    "    \n",
    "    evl(model=model,q_set=q_loader,c_set=c_loader,  # model and data\n",
    "          bs = batch_size\n",
    "        ,\n",
    "          embed_size=embed_size,loss = loss,metric = metric,sampler =sampler,num_cls = num_cls,   # train\n",
    "          save=save,result_f=result,model_f=model_name  # log\n",
    "          )\n",
    "    print('Done')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(test)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
